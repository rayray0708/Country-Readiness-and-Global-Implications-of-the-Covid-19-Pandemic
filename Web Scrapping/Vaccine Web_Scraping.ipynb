{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping - Project 3\n",
    "## https://covid19.who.int/table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Splinter and BeautifulSoup\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup Splinter\n",
    "browser = Browser('chrome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visit the Mars news site\n",
    "url = 'https://covid19.who.int/table'\n",
    "browser.visit(url)\n",
    "html = browser.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Scrape the Website\n",
    "\n",
    "Create a Beautiful Soup object and use it to extract text elements from the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Beautiful Soup object\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<span>+ By WHO Region</span>, <span>+ By World Bank Income Group</span>, <span>United States of America</span>, <span>China</span>, <span>India</span>, <span>France</span>, <span>Germany</span>, <span>Brazil</span>, <span>Republic of Korea</span>, <span>Japan</span>, <span>Italy</span>, <span>The United Kingdom</span>, <span>Russian Federation</span>, <span>TÃ¼rkiye</span>, <span>Spain</span>, <span>Australia</span>, <span>Viet Nam</span>, <span>Argentina</span>, <span>Netherlands</span>, <span>Mexico</span>]\n"
     ]
    }
   ],
   "source": [
    "# Extract all the text elements\n",
    "article = soup.find_all('div', class_='tbody')\n",
    "article2 = article[0].find_all('span')\n",
    "print(article2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
